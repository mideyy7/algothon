# standardize_3assets_pca_corr.py
# Input: a pandas DataFrame with timestamp index and 3 asset columns (e.g., ["thames", "weather", "airport"])
# Output: standardized data, PCA components/scores, and correlation analysis

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

ASSET_COLS = ["thames", "weather", "airport"]  # <- change to your 3 column names


def standardize_assets(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    x = df[cols].astype(float).values
    scaler = StandardScaler()
    z = scaler.fit_transform(x)
    z_df = pd.DataFrame(z, index=df.index, columns=[f"{c}_z" for c in cols])
    return z_df


def run_pca(z_df: pd.DataFrame, n_components: int = 3) -> dict:
    pca = PCA(n_components=n_components)
    scores = pca.fit_transform(z_df.values)

    scores_df = pd.DataFrame(
        scores,
        index=z_df.index,
        columns=[f"PC{i+1}" for i in range(scores.shape[1])],
    )

    loadings_df = pd.DataFrame(
        pca.components_.T,
        index=z_df.columns,
        columns=[f"PC{i+1}" for i in range(pca.components_.shape[0])],
    )

    explained = pd.Series(
        pca.explained_variance_ratio_,
        index=[f"PC{i+1}" for i in range(len(pca.explained_variance_ratio_))],
        name="explained_var_ratio",
    )

    return {
        "pca": pca,
        "scores": scores_df,
        "loadings": loadings_df,
        "explained": explained,
    }


def corr_analysis(z_df: pd.DataFrame, pca_scores: pd.DataFrame) -> dict:
    corr_assets = z_df.corr()
    corr_pc = pca_scores.corr()

    # correlation between standardized assets and PCs
    corr_assets_to_pc = pd.concat([z_df, pca_scores], axis=1).corr().loc[z_df.columns, pca_scores.columns]

    return {
        "corr_assets": corr_assets,
        "corr_pcs": corr_pc,
        "corr_assets_to_pcs": corr_assets_to_pc,
    }


def main(df: pd.DataFrame, cols: list[str] = ASSET_COLS) -> dict:
    # 1) keep only rows where all 3 assets exist
    df = df.copy()
    df = df.dropna(subset=cols)

    # 2) standardize
    z_df = standardize_assets(df, cols)

    # 3) PCA
    pca_out = run_pca(z_df, n_components=len(cols))

    # 4) correlations
    corr_out = corr_analysis(z_df, pca_out["scores"])

    return {
        "z": z_df,
        "pca_scores": pca_out["scores"],
        "pca_loadings": pca_out["loadings"],
        "pca_explained": pca_out["explained"],
        **corr_out,
    }


# ----------- Example usage -----------
if __name__ == "__main__":
    # Example dummy data (replace with your real dataframe)
    # df must have columns matching ASSET_COLS and a timestamp index (optional but recommended).
    t = pd.date_range("2026-02-28 12:00", periods=100, freq="15min")
    df = pd.DataFrame(
        {
            "thames": np.cumsum(np.random.randn(len(t))) + 1000,
            "weather": np.cumsum(np.random.randn(len(t))) + 1500,
            "airport": np.cumsum(np.random.randn(len(t))) + 500,
        },
        index=t,
    )

    out = main(df)

    print("\n--- PCA explained variance ratio ---")
    print(out["pca_explained"])

    print("\n--- PCA loadings (how each asset contributes to each PC) ---")
    print(out["pca_loadings"])

    print("\n--- Correlation of standardized assets ---")
    print(out["corr_assets"])

    print("\n--- Correlation between assets and PCs ---")
    print(out["corr_assets_to_pcs"])
