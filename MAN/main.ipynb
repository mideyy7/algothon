{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db10e1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:07.427387Z",
     "iopub.status.busy": "2026-02-28T18:04:07.427143Z",
     "iopub.status.idle": "2026-02-28T18:04:08.859210Z",
     "shell.execute_reply": "2026-02-28T18:04:08.858687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: 2017-01-03 00:00:00 -> 2025-09-30 00:00:00 | Assets: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATA_DIR = \"data/2025-09-30\"\n",
    "round = 'round_5'\n",
    "\n",
    "prices  = pd.read_csv(f\"{DATA_DIR}/prices.csv\",     parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "volumes = pd.read_csv(f\"{DATA_DIR}/volumes.csv\",    parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "signals = pd.read_csv(f\"{DATA_DIR}/signals.csv\",    parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "cash    = pd.read_csv(f\"{DATA_DIR}/cash_rate.csv\",  parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "\n",
    "# Align assets & dates across the main panels\n",
    "common_assets = sorted(set(prices.columns) & set(volumes.columns) & set(signals.columns))\n",
    "common_dates = prices.index.intersection(volumes.index).intersection(signals.index)\n",
    "\n",
    "print(\"Dates:\", prices.index.min(), \"->\", prices.index.max(), \"| Assets:\", len(common_assets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d37efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:08.860804Z",
     "iopub.status.busy": "2026-02-28T18:04:08.860665Z",
     "iopub.status.idle": "2026-02-28T18:04:08.870552Z",
     "shell.execute_reply": "2026-02-28T18:04:08.870085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned shapes -> prices: (3124, 10) | volumes_renamed: (3124, 10) | signals: (3124, 40)\n",
      "Date range: 2017-01-03 00:00:00 -> 2025-09-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Assets come from prices.csv\n",
    "assets = list(prices.columns)\n",
    "assert len(assets) > 0, \"prices has zero asset columns.\"\n",
    "\n",
    "# Align to common dates across prices/volumes/signals\n",
    "common_dates = prices.index.intersection(volumes.index).intersection(signals.index)\n",
    "prices  = prices.loc[common_dates, assets]\n",
    "volumes = volumes.loc[common_dates]\n",
    "signals = signals.loc[common_dates]\n",
    "\n",
    "# volumes.csv has columns like INSTRUMENT_1_vol -> rename back to INSTRUMENT_1\n",
    "vol_cols = [f\"{a}_vol\" for a in assets]\n",
    "missing_vol = [c for c in vol_cols if c not in volumes.columns]\n",
    "assert len(missing_vol) == 0, f\"Missing volume columns like: {missing_vol[:10]}\"\n",
    "\n",
    "volumes_renamed = volumes[vol_cols].copy()\n",
    "volumes_renamed.columns = assets\n",
    "\n",
    "# signals.csv has INSTRUMENT_k_trend{4,8,16,32}\n",
    "trend_horizons = [4, 8, 16, 32]\n",
    "sig_trend = {}\n",
    "for h in trend_horizons:\n",
    "    cols = [f\"{a}_trend{h}\" for a in assets]\n",
    "    missing = [c for c in cols if c not in signals.columns]\n",
    "    assert len(missing) == 0, f\"Missing trend columns for trend{h}, e.g. {missing[:10]}\"\n",
    "    tmp = signals[cols].copy()\n",
    "    tmp.columns = assets\n",
    "    sig_trend[f\"trend{h}\"] = tmp\n",
    "\n",
    "print(\"Aligned shapes -> prices:\", prices.shape, \"| volumes_renamed:\", volumes_renamed.shape, \"| signals:\", signals.shape)\n",
    "print(\"Date range:\", prices.index.min(), \"->\", prices.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fed0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:08.871860Z",
     "iopub.status.busy": "2026-02-28T18:04:08.871757Z",
     "iopub.status.idle": "2026-02-28T18:04:08.875944Z",
     "shell.execute_reply": "2026-02-28T18:04:08.875485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret1 shape: (3124, 10) | y shape: (3124, 10)\n"
     ]
    }
   ],
   "source": [
    "logp = np.log(prices)\n",
    "ret1 = logp.diff()\n",
    "\n",
    "H = 60\n",
    "# Forward 60 trading-day log return: sum_{t+1..t+60} r\n",
    "y = ret1.rolling(H).sum().shift(-H)\n",
    "\n",
    "print(\"ret1 shape:\", ret1.shape, \"| y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3966da78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:08.877226Z",
     "iopub.status.busy": "2026-02-28T18:04:08.877135Z",
     "iopub.status.idle": "2026-02-28T18:04:39.193382Z",
     "shell.execute_reply": "2026-02-28T18:04:39.192615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 88\n"
     ]
    }
   ],
   "source": [
    "feat_dict = {}\n",
    "\n",
    "# momentum base\n",
    "mom_3  = ret1.rolling(3).sum()\n",
    "mom_10 = ret1.rolling(10).sum()\n",
    "mom_20 = ret1.rolling(20).sum()\n",
    "\n",
    "# momentum extra windows\n",
    "mom_windows_extra = [30, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200, 220, 240, 260]\n",
    "mom_extra = {f\"mom_{w}\": ret1.rolling(w).sum() for w in mom_windows_extra}\n",
    "\n",
    "# reversal\n",
    "rev_1 = -ret1\n",
    "\n",
    "# vol base\n",
    "vol_10 = ret1.rolling(10).std()\n",
    "vol_20 = ret1.rolling(20).std()\n",
    "\n",
    "# vol extra\n",
    "vol_windows_extra = [30, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200, 220, 240, 260]\n",
    "vol_extra = {f\"vol_{w}\": ret1.rolling(w).std() for w in vol_windows_extra}\n",
    "\n",
    "# volume z-score (handle zeros safely)\n",
    "volumes_safe = volumes_renamed.replace(0, np.nan)\n",
    "volz_20 = (volumes_safe - volumes_safe.rolling(20).mean()) / (volumes_safe.rolling(20).std() + 1e-12)\n",
    "\n",
    "# cross-sectional dispersion replicated across assets\n",
    "disp = ret1.std(axis=1)\n",
    "dispersion = pd.DataFrame(np.repeat(disp.values[:, None], len(assets), axis=1),\n",
    "                          index=prices.index, columns=assets)\n",
    "\n",
    "# relative features\n",
    "rel_mom_10 = mom_10.sub(mom_10.mean(axis=1), axis=0)\n",
    "rel_ret_1  = ret1.sub(ret1.mean(axis=1), axis=0)\n",
    "\n",
    "# avg correlation vs market proxy\n",
    "market = ret1.mean(axis=1)\n",
    "avg_corr = pd.DataFrame(index=prices.index, columns=assets, dtype=float)\n",
    "window_corr = 60\n",
    "for a in assets:\n",
    "    avg_corr[a] = ret1[a].rolling(window_corr).corr(market)\n",
    "\n",
    "# rolling PCA features (factor, loading, residual) for multiple windows\n",
    "def rolling_pca_features(ret_df: pd.DataFrame, window: int):\n",
    "    dates = ret_df.index\n",
    "    assets_all = ret_df.columns\n",
    "\n",
    "    pc1_factor  = pd.DataFrame(np.nan, index=dates, columns=assets_all)\n",
    "    pc1_loading = pd.DataFrame(np.nan, index=dates, columns=assets_all)\n",
    "    pc1_resid_1 = pd.DataFrame(np.nan, index=dates, columns=assets_all)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "\n",
    "    for i in range(window, len(dates)):\n",
    "        window_slice = ret_df.iloc[i-window:i].dropna(axis=1, how=\"any\")\n",
    "        if window_slice.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        X = window_slice.values\n",
    "        X = X - X.mean(axis=0, keepdims=True)\n",
    "\n",
    "        pca.fit(X)\n",
    "        x_last = X[-1:]\n",
    "        score = pca.transform(x_last)[0, 0]\n",
    "        loadings = pca.components_[0]\n",
    "\n",
    "        recon = score * loadings\n",
    "        resid = x_last.flatten() - recon\n",
    "\n",
    "        slice_assets = window_slice.columns\n",
    "        pc1_factor.loc[dates[i], slice_assets] = score\n",
    "        pc1_loading.loc[dates[i], slice_assets] = loadings\n",
    "        pc1_resid_1.loc[dates[i], slice_assets] = resid\n",
    "\n",
    "    return pc1_factor, pc1_loading, pc1_resid_1\n",
    "\n",
    "pca_windows = [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 300]\n",
    "pca_factor_feats, pca_loading_feats, pca_resid_feats = {}, {}, {}\n",
    "\n",
    "for w in pca_windows:\n",
    "    fac_df, load_df, resid_df = rolling_pca_features(ret1, window=w)\n",
    "    pca_factor_feats[f\"pc1_factor_w{w}\"]   = fac_df\n",
    "    pca_loading_feats[f\"pc1_loading_w{w}\"] = load_df\n",
    "    pca_resid_feats[f\"pc1_resid_1_w{w}\"]   = resid_df\n",
    "\n",
    "# cash3mo macro replicated across assets (your cash_rate.csv has '3mo')\n",
    "cash_ff = cash.reindex(prices.index).ffill()\n",
    "cash3mo = cash_ff[\"3mo\"]\n",
    "macro_3mo = pd.DataFrame(np.repeat(cash3mo.values[:, None], len(assets), axis=1),\n",
    "                         index=prices.index, columns=assets)\n",
    "\n",
    "# assemble full feat_dict\n",
    "feat_dict = {\n",
    "    \"rel_mom_10\": rel_mom_10,\n",
    "    \"rel_ret_1\": rel_ret_1,\n",
    "    \"avg_corr\": avg_corr,\n",
    "\n",
    "    \"mom_3\": mom_3,\n",
    "    \"mom_10\": mom_10,\n",
    "    \"mom_20\": mom_20,\n",
    "    \"rev_1\": rev_1,\n",
    "    \"vol_10\": vol_10,\n",
    "    \"vol_20\": vol_20,\n",
    "    \"volz_20\": volz_20,\n",
    "    \"disp\": dispersion,\n",
    "\n",
    "    \"trend4\": sig_trend[\"trend4\"],\n",
    "    \"trend8\": sig_trend[\"trend8\"],\n",
    "    \"trend16\": sig_trend[\"trend16\"],\n",
    "    \"trend32\": sig_trend[\"trend32\"],\n",
    "\n",
    "    \"cash3mo\": macro_3mo,\n",
    "}\n",
    "\n",
    "feat_dict.update(mom_extra)\n",
    "feat_dict.update(vol_extra)\n",
    "feat_dict.update(pca_factor_feats)\n",
    "feat_dict.update(pca_loading_feats)\n",
    "feat_dict.update(pca_resid_feats)\n",
    "\n",
    "print(\"Total features:\", len(feat_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56cc66e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:39.197299Z",
     "iopub.status.busy": "2026-02-28T18:04:39.197145Z",
     "iopub.status.idle": "2026-02-28T18:04:39.624298Z",
     "shell.execute_reply": "2026-02-28T18:04:39.623843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_panel_all shape: (31240, 88) | #features: 88\n"
     ]
    }
   ],
   "source": [
    "# Build wide df with MultiIndex columns: (feature, asset)\n",
    "X_wide = pd.concat(feat_dict, axis=1)  # (feature, asset)\n",
    "\n",
    "# Convert to panel with rows (date, asset) and columns feature\n",
    "# Since our concat is (feature, asset), stacking level=1 will stack the asset level.\n",
    "X_panel_all = X_wide.stack(level=1)\n",
    "X_panel_all.index.names = [\"date\", \"asset\"]  # (date, asset) index\n",
    "# columns are feature names now\n",
    "\n",
    "# Replace inf -> NaN\n",
    "X_panel_all = X_panel_all.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Cross-sectional rank per date\n",
    "X_panel_all = X_panel_all.groupby(level=0).rank(pct=True)\n",
    "\n",
    "# Fill NaNs per date with per-date median (transform keeps same index always)\n",
    "med = X_panel_all.groupby(level=0).transform(\"median\")\n",
    "X_panel_all = X_panel_all.fillna(med)\n",
    "\n",
    "# Any remaining NaNs (e.g., all NaN on that date/feature) -> neutral 0.5\n",
    "X_panel_all = X_panel_all.fillna(0.5)\n",
    "\n",
    "feature_cols = list(X_panel_all.columns)\n",
    "assert len(feature_cols) > 0, \"feature_cols ended up empty (should not happen with this data).\"\n",
    "\n",
    "print(\"X_panel_all shape:\", X_panel_all.shape, \"| #features:\", len(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7059eefb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:39.625583Z",
     "iopub.status.busy": "2026-02-28T18:04:39.625497Z",
     "iopub.status.idle": "2026-02-28T18:04:39.642667Z",
     "shell.execute_reply": "2026-02-28T18:04:39.642293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel_train shape: (30640, 89) | trainable dates: 3064\n",
      "Trainable date range: 2017-01-03 00:00:00 -> 2025-08-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "y_panel = y.stack().rename(\"y\")\n",
    "y_panel.index.names = [\"date\", \"asset\"]\n",
    "\n",
    "panel_train = X_panel_all.join(y_panel, how=\"inner\").dropna(subset=[\"y\"])\n",
    "\n",
    "dates = panel_train.index.get_level_values(0).unique().sort_values()\n",
    "print(\"panel_train shape:\", panel_train.shape, \"| trainable dates:\", len(dates))\n",
    "print(\"Trainable date range:\", dates.min(), \"->\", dates.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "333123da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:39.644170Z",
     "iopub.status.busy": "2026-02-28T18:04:39.644038Z",
     "iopub.status.idle": "2026-02-28T18:04:39.684356Z",
     "shell.execute_reply": "2026-02-28T18:04:39.683873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train last date: 2023-11-27 00:00:00\n",
      "Predict date: 2025-09-30 00:00:00\n",
      "asset\n",
      "INSTRUMENT_7     0.114257\n",
      "INSTRUMENT_9     0.064927\n",
      "INSTRUMENT_10    0.035492\n",
      "INSTRUMENT_2     0.021769\n",
      "INSTRUMENT_5     0.006207\n",
      "INSTRUMENT_3    -0.013325\n",
      "INSTRUMENT_1    -0.020425\n",
      "INSTRUMENT_6    -0.029810\n",
      "INSTRUMENT_8    -0.033218\n",
      "INSTRUMENT_4    -0.034371\n",
      "Name: mu_hat, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "split = int(len(dates) * 0.8)\n",
    "train_dates = dates[:split]\n",
    "train = panel_train.loc[train_dates]\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(train[feature_cols].to_numpy(dtype=float), train[\"y\"].to_numpy(dtype=float))\n",
    "\n",
    "latest_date = X_panel_all.index.get_level_values(0).max()\n",
    "latest_X = X_panel_all.loc[latest_date, feature_cols]  # index=asset\n",
    "\n",
    "mu_hat = pd.Series(model.predict(latest_X.to_numpy(dtype=float)), index=latest_X.index, name=\"mu_hat\")\n",
    "\n",
    "print(\"Train last date:\", train_dates.max())\n",
    "print(\"Predict date:\", latest_date)\n",
    "print(mu_hat.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda9880e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:39.686351Z",
     "iopub.status.busy": "2026-02-28T18:04:39.686222Z",
     "iopub.status.idle": "2026-02-28T18:04:39.695952Z",
     "shell.execute_reply": "2026-02-28T18:04:39.695486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: AAKK_round_5.csv\n",
      "Sum weights: 1.0 Min weight: 0.0 (can be 0 now)\n",
      "           asset    weight\n",
      "6   INSTRUMENT_7  0.531870\n",
      "1   INSTRUMENT_2  0.180075\n",
      "8   INSTRUMENT_9  0.164758\n",
      "4   INSTRUMENT_5  0.065826\n",
      "9  INSTRUMENT_10  0.057472\n",
      "0   INSTRUMENT_1  0.000000\n",
      "2   INSTRUMENT_3  0.000000\n",
      "3   INSTRUMENT_4  0.000000\n",
      "5   INSTRUMENT_6  0.000000\n",
      "7   INSTRUMENT_8  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Risk-aware score ~ Sharpe proxy: mu / vol\n",
    "VOL_WIN = 20\n",
    "vol = ret1[mu_hat.index].rolling(VOL_WIN).std().loc[latest_date]\n",
    "score = mu_hat / (vol + 1e-8)\n",
    "score = score.replace([np.inf, -np.inf], np.nan).fillna(score.median())\n",
    "\n",
    "# Long-only weights with true zeros: center then clip\n",
    "s = score - score.median()\n",
    "w = s.clip(lower=0.0)\n",
    "\n",
    "# fallback if everything <= 0\n",
    "if w.sum() <= 0:\n",
    "    w = pd.Series(1.0, index=score.index)\n",
    "\n",
    "w = w / w.sum()\n",
    "\n",
    "submission = pd.DataFrame({\"asset\": w.index, \"weight\": w.values}).sort_values(\"weight\", ascending=False)\n",
    "submission.to_csv(f\"submissions/AAKK_{round}.csv\", index=False)\n",
    "\n",
    "print(f\"Saved: AAKK_{round}.csv\")\n",
    "print(\"Sum weights:\", submission[\"weight\"].sum(), \"Min weight:\", submission[\"weight\"].min(), \"(can be 0 now)\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39a8e9ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T18:04:39.697094Z",
     "iopub.status.busy": "2026-02-28T18:04:39.696990Z",
     "iopub.status.idle": "2026-02-28T18:04:39.699289Z",
     "shell.execute_reply": "2026-02-28T18:04:39.698859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y vs manual: -0.03421587763547951 -0.03421587763547951\n"
     ]
    }
   ],
   "source": [
    "# pick a date/asset where label exists and verify y = sum of next 60 daily returns\n",
    "t = dates[-200]\n",
    "a = assets[0]\n",
    "manual = ret1[a].loc[t:].iloc[1:61].sum()\n",
    "print(\"y vs manual:\", float(y.loc[t, a]), float(manual))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
